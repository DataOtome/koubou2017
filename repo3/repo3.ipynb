{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最もよく使うLoss関数の一つであるSoftmax with lossレイヤーを実装してください．\n",
    "特にBackwardを自分で実装してみてください。式の導出は難しいので省略してokです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "   # 教師データがone-hot-vectorの場合、正解ラベルのインデックスに変換\n",
    "    if  t.size == y.size:\n",
    "        t = t.argmax(axis=1)#正解的索引下标\n",
    "        batch_size = y.shape[0]\n",
    "        return -np.sum(np.log(y[np.arange(batch_size), t])) / batch_size\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None # softmaxの出力\n",
    "        self.t = None # 教師データ\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "       # forwardの式\n",
    "       # -sum ( t * log (y))\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        return self.loss\n",
    "    def backward(self, dout=1):\n",
    "       # backwardの式\n",
    "       # yi - ti (iはIndex)\n",
    "        batch_size = self.t.shape[0]\n",
    "       # Backwardを実装して、微分値をdxに代入してください\n",
    "        dx = self.y - self.x\n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "私が知っているSoftmaxの式はexp(yi)/sum(exp(y))だったが、このサンプルコードの式はlog(yi)/sum(log(y))だ\n",
    "コメントに\n",
    "# backwardの式\n",
    "# yi - ti (iはIndex)\n",
    "を書いていたからそれをしたかってコードにした"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下記のサンプルコードを実行してTwo layer netにおける勾配の確認をしてみてください．十分小さな値になればOKです.\n",
    "http://pr.cei.uec.ac.jp/kobo2017/index.php?source のコードを適宜使ってください."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "問題の意味を理解できない。サンプルコードは壊れてたもの。例えばサンプルコードの中の行def gradient(self, x, t):、self.loss(x, t)。これは明らかにclassの内部のコード。でも元のclassの構成がわからない以上、値を取らないself.loss(x, t)行の命令は何を意味するのはわからない。\n",
    "だから提供されたコードを実行してみた"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W1': array([[-0.00836851, -0.00172161, -0.00541089],\n",
      "       [-0.00975142,  0.0077424 , -0.00594833],\n",
      "       [ 0.00333508,  0.00469917,  0.00564827]]), 'b1': array([ 0.,  0.,  0.]), 'W2': array([[ 0.01221612,  0.00899016, -0.00045972, -0.00485793],\n",
      "       [-0.00287717, -0.00299911, -0.002049  , -0.00132289],\n",
      "       [-0.00438003,  0.00663949, -0.00972822, -0.00597882]]), 'b2': array([ 0.,  0.,  0.,  0.])}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "def numerical_grad(f, x):\n",
    "   h = 1e-4 # 0.0001\n",
    "   grad = np.zeros_like(x)\n",
    "   it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "   while not it.finished:\n",
    "       idx = it.multi_index\n",
    "       tmp_val = x[idx]\n",
    "       x[idx] = float(tmp_val) + h\n",
    "       fxh1 = f(x) # f(x+h)\n",
    "       x[idx] = tmp_val - h \n",
    "       fxh2 = f(x) # f(x-h)\n",
    "       grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "       x[idx] = tmp_val # 値を元に戻す\n",
    "       it.iternext()   \n",
    "   return grad\n",
    "class TwoLayerNet:\n",
    "   def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
    "       # 重みの初期化\n",
    "       self.params = {}\n",
    "       self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "       self.params['b1'] = np.zeros(hidden_size)\n",
    "       self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size) \n",
    "       self.params['b2'] = np.zeros(output_size)\n",
    "       # レイヤの生成\n",
    "       self.layers = OrderedDict()\n",
    "       self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "       self.layers['Relu1'] = Relu()\n",
    "       self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "       self.lastLayer = SoftmaxWithLoss()        \n",
    "   def predict(self, x):\n",
    "       for layer in self.layers.values():\n",
    "           x = layer.forward(x)\n",
    "       return x\n",
    "   # x:入力データ, t:教師データ\n",
    "   def loss(self, x, t):\n",
    "       y = self.predict(x)\n",
    "       return self.lastLayer.forward(y, t)\n",
    "   def numerical_gradient(self, x, t):\n",
    "       loss_W = lambda W: self.loss(x, t)\n",
    "       grads = {}\n",
    "       grads['W1'] = numerical_grad(loss_W, self.params['W1'])\n",
    "       grads['b1'] = numerical_grad(loss_W, self.params['b1'])\n",
    "       grads['W2'] = numerical_grad(loss_W, self.params['W2'])\n",
    "       grads['b2'] = numerical_grad(loss_W, self.params['b2'])\n",
    "       return grads\n",
    "\n",
    "A=TwoLayerNet(3,3,4)\n",
    "print(A.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下記のサンプルコードを実行してTwo layer netの学習をしてみてください．ランダムな場合の精度が10％なのでそれ以上の精度が出るはずです.\n",
    "http://pr.cei.uec.ac.jp/kobo2017/index.php?source のコードを適宜使ってください."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "サンプルコードが使えないため学習できない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'f' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-ef4a561e859f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m    \u001b[1;31m# 勾配\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m    \u001b[1;31m#grad = network.numerical_gradient(x_batch, t_batch)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m    \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m    \u001b[1;31m# 更新\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m    \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'W1'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'b1'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'W2'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'b2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-41-8c91ca0a1871>\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(x, t)\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mtmp_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp_val\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mfxh1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# f(x+h)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtmp_val\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mfxh2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# f(x-h)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'f' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "# データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "for i in range(iters_num):\n",
    "   batch_mask = np.random.choice(train_size, batch_size)\n",
    "   x_batch = x_train[batch_mask]\n",
    "   t_batch = t_train[batch_mask]\n",
    "   # 勾配\n",
    "   #grad = network.numerical_gradient(x_batch, t_batch)\n",
    "   grad = gradient(x_batch, t_batch)\n",
    "   # 更新\n",
    "   for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "       network.params[key] -= learning_rate * grad[key]\n",
    "   loss = network.loss(x_batch, t_batch)\n",
    "   train_loss_list.append(loss)\n",
    "   if i % iter_per_epoch == 0:\n",
    "       train_acc = network.accuracy(x_train, t_train)\n",
    "       test_acc = network.accuracy(x_test, t_test)\n",
    "       train_acc_list.append(train_acc)\n",
    "       test_acc_list.append(test_acc)\n",
    "       print(train_acc, test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
